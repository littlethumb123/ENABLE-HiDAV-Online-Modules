{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Welcome to the third text mining module. In this module, you will learn about Part of Speech Tagging, Named Entity Recognition, and Relation Extraction. Under each session, you will have a short tutorial which shows you how to complete an information extraction task using text mining tool. The goal is to give you hands-on experience on extracting key information using a text mining tool. Section 2, 3, and 4.1, have optional exercises to allow you to get familair with the concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## How to Run the Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Throughout this module you will encounter both text and code cells. Please run each cell in this Notebook by clicking \"Run\" button in the Toolbar or by pushing Shift+Enter keys\n",
    "<br>\n",
    "![run_cell.png](attachment:run_cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Part of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"705\" height=\"537\" src=\"https://www.youtube.com/embed/VbmWA_32l9g\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "# Display Video\n",
    "HTML('<iframe width=\"705\" height=\"537\" src=\"https://www.youtube.com/embed/VbmWA_32l9g\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Task 1: POS tags and Noun phrase extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the following, we will take a taste of parsing a text and complete the following tasks\n",
    "* Tag each word with Penn tree bank and universal tags\n",
    "* Extract noun phrases\n",
    "\n",
    "In this module, you will use spaCy to complete the following tasks. You already have had some experience with NLTK in tokenization, lemmatization, stemming. spaCy is a another very useful NLP tool and a strong competiter to NLTK. Unlike NLTK, spaCy takes over the dirty work using an object-oriented approach, which makes text processing easier and faster. For example, it extracts noun phrases without the need to pre-design regex pattern and traverse the parser tree. Instead, you simply \"call\" the noun chunks that spacy has done for you under the hood. \n",
    "\n",
    "Here is a simple code example to extract the noun phrases using spaCy:\n",
    "\n",
    "```python\n",
    "doc = nlp(string_to_process)\n",
    "for np in doc.noun_chunks:\n",
    "    print(np.text)\n",
    "```\n",
    "\n",
    "For additional resources for spaCy 101 tutorial, you can find [here](https://spacy.io/usage/spacy-101). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### How to solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Import spacy package and load english language model\n",
    "* Get the spacy object of such text\n",
    "* Obtain the pos tagging attributes of each word, word.pos_ and noun chunk results noun.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy                    #import spacy module\n",
    "import en_core_web_sm\n",
    "\n",
    "# load English language model\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "TEXT_SAMPLE = \"\"\"Health informatics is information engineering applied to the field of health care, essentially the management and use of patient health care information \"\"\"\n",
    "\n",
    "doc = nlp(TEXT_SAMPLE)\n",
    "\n",
    "# Obtain tags using pos_tag\n",
    "print(\"The pos tags of the given text:\")\n",
    "for tok in doc:\n",
    "    print((tok.text, tok.tag_))\n",
    "\n",
    "print(\"*\"*20)          \n",
    "print(\"The universal pos tags of the given text\")\n",
    "print(\"*\"*20) \n",
    "for tok in doc:\n",
    "    print((tok.text, tok.pos_))\n",
    "\n",
    "print(\"*\"*20)\n",
    "print(\"The extracted noun phrases is: \")\n",
    "print(\"*\"*20) \n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Practice Exercise (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on the task above, can you parse the following text and text the noun phrases from that? You can also choose other tools and text that you are interested in to do the exercise. \n",
    "\n",
    "    \"Information science is that discipline that investigates the properties and behavior of information, the forces governing the flow of information, and the means of processing information for optimum accessibility and usability.\"\n",
    "\n",
    "-Borko, H. (1968). Information science: What is it? American Documentation, 19, 3. Retrieved from ASIST, [What is information science](https://www.asist.org/about/what-is-information-science/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy                    \n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "text_1 = \"\"\"Information science is the discipline that investigates the properties and behavior of information, \n",
    "            the forces governing the flow of information, and the means of processing information for \n",
    "            optimum accessibility and usability.\"\"\"\n",
    "\n",
    "# here is your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"705\" height=\"537\" src=\"https://www.youtube.com/embed/QK4gxqfWsbE\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"705\" height=\"537\" src=\"https://www.youtube.com/embed/QK4gxqfWsbE\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##  Task 2: Extract Named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we will use spaCy to extract **Organization**, **Geopolitical entity**, **CARDINAL**, **Money** entities from the text. \n",
    "For Named entity uses OntoNotes 5 corpus to train its entity recognition model, spaCy supporting the detection of a wider variety of entities than NLTK. This corpus includes diverse data sources, e.g., telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs. More detail can be found [here](https://catalog.ldc.upenn.edu/LDC2013T19).   \n",
    "\n",
    "* *The NIH was founded in 1887 and is now part of the United States Department of Health and Human Services.* \n",
    "* *The NIH is located in Maryland, U.S. and has nearly 1,000 scientists and support staff.*\n",
    "* *The NIH obtained US$39 billion from Congress in 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### How to solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Import spacy and english model\n",
    "* Using nlp() to obtain the attribute of each word in the text. nlp() helps you detect and extract the name entities. For more detail about what you can do with nlp(), you can find [here](https://spacy.io/usage/spacy-101)\n",
    "* You can either use doc.ents    \n",
    "```python\n",
    "doc = nlp(string_to_process)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label)\n",
    "```\n",
    "    or get the words' attributes \"ent_type_\" to obtain named entities.\n",
    "```python\n",
    "doc = nlp(string_to_process)\n",
    "for token in doc:\n",
    "    print(token.ent_type_)\n",
    "```\n",
    "\n",
    "* Visualize the entities within the sentence with displacy\n",
    "```python\n",
    "displacy.render(doc, style=\"ent\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Using doc.ent to directly get entities\n",
      "********************\n",
      "Marc Lipsitch: 0, 13, PERSON\n",
      "Harvard: 17, 24, ORG\n",
      "the Center for Communicable Disease Dynamics: 71, 115, ORG\n",
      "one: 125, 128, CARDINAL\n",
      "first: 136, 141, ORDINAL\n",
      "U.S.: 169, 173, GPE\n",
      "********************\n",
      "Using doc.token to directly get entities\n",
      "********************\n",
      "We are processing the text:  Marc Lipsitch, a Harvard professor of epidemiology and the director of the Center for Communicable Disease Dynamics, created one of the first modeling tools used in the U.S. for the COVID-19 pandemic\n",
      "Marc: PERSON\n",
      "Lipsitch: PERSON\n",
      ",: \n",
      "a: \n",
      "Harvard: ORG\n",
      "professor: \n",
      "of: \n",
      "epidemiology: \n",
      "and: \n",
      "the: \n",
      "director: \n",
      "of: \n",
      "the: ORG\n",
      "Center: ORG\n",
      "for: ORG\n",
      "Communicable: ORG\n",
      "Disease: ORG\n",
      "Dynamics: ORG\n",
      ",: \n",
      "created: \n",
      "one: CARDINAL\n",
      "of: \n",
      "the: \n",
      "first: ORDINAL\n",
      "modeling: \n",
      "tools: \n",
      "used: \n",
      "in: \n",
      "the: \n",
      "U.S.: GPE\n",
      "for: \n",
      "the: \n",
      "COVID-19: \n",
      "pandemic: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Marc Lipsitch\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", a \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Harvard\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " professor of epidemiology and the director of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Center for Communicable Disease Dynamics\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", created \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " modeling tools used in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " for the COVID-19 pandemic</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy                    #import spacy module\n",
    "from spacy import displacy         # import NER visualizer\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "TEXT_SAMPLE = \"\"\"\n",
    "The NIH was founded in April 1887 and is now part of the United States Department of Health and Human Services.\n",
    "The NIH is located in Maryland, U.S. and contains nearly 1,000 scientists and support staff.\n",
    "The NIH obtained US$39 billion from Congress in 2019.\n",
    "\"\"\"\n",
    "\n",
    "# load English language model\n",
    "nlp = en_core_web_sm.load()\n",
    "# pass the text to nlp\n",
    "\n",
    "doc = nlp(\"Marc Lipsitch, a Harvard professor of epidemiology and the director of the Center for Communicable Disease Dynamics, created one of the first modeling tools used in the U.S. for the COVID-19 pandemic\")\n",
    "\n",
    "# Extract the entities from such doc objects. We will get the following attributes of\n",
    "# the entity, i.e., original text, start position, end position, entity type\n",
    "print(\"*\"*20) \n",
    "print(\"Using doc.ent to directly get entities\")\n",
    "print(\"*\"*20) \n",
    "for ent in doc.ents:\n",
    "    print(\"{}: {}, {}, {}\".format(ent.text,      # original text\n",
    "                                ent.start_char,  # start position\n",
    "                                ent.end_char,    # end position\n",
    "                                ent.label_))         # entity type\n",
    "    \n",
    "# You can also access the attributes of each token directly. \n",
    "# Here we obtian the text, pos_tags, IOB entity label and named entity label.\n",
    "print(\"*\"*20) \n",
    "print(\"Using doc.token to directly get entities\")\n",
    "print(\"*\"*20) \n",
    "for sent in doc.sents:\n",
    "    print(\"We are processing the text: \", sent.text)      # print the sentence\n",
    "    for tok in sent:\n",
    "        print(\"{}: {}\".format(tok.text, tok.ent_type_))      # original text and entity type\n",
    "                                          \n",
    "# Visualize the entities displacy by specifying the visualization type as \"ent\"     \n",
    "displacy.render(doc, style=\"ent\")\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Practice Exercise (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on the task above, can you extract the PERSON, ORGANIZATION,  from the given text. You can also choose other tools and text that you are interested in to do the exercise. \n",
    "\n",
    "    \"Marc Lipsitch, a Harvard professor of epidemiology and the director of the Center for Communicable Disease Dynamics, created one of the first modeling tools used in the U.S. for the COVID-19 pandemic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy                    #import spacy module\n",
    "from spacy import displacy         # import NER visualizer\n",
    "import en_core_web_sm\n",
    "\n",
    "text = \"\"\"Marc Lipsitch, a Harvard professor of epidemiology and the director of the Center for Communicable Disease Dynamics, \n",
    "            created one of the first modeling tools used in the U.S. for the COVID-19 pandemic\"\"\"\n",
    "# here is your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the previous sections, we are still analyzing text at a lexical level, i.e, the predefined label of words. In order to obtain more information from the text, syntactax structure analysis comes to play and provides us with the syntactic information in the text. The syntactax structure describes the arrangement of words and phrases to create well-formed sentences in a language. \n",
    "In this section, we will first look at the syntactic dependency structure, that is, how words group as a unit and how the units relate to each other. We will then introduce the universal dependency and end this section up with the extraction of subject-predicate-object relation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.1 Syntactic Dependency Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"705\" height=\"537\" src=\"https://www.youtube.com/embed/7rR-Z8jGc7c\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"705\" height=\"537\" src=\"https://www.youtube.com/embed/7rR-Z8jGc7c\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1.2 Additional Resources for the Dependency Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4.1.2.1 Universal dependency structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One thing we didn't mention in the video is the [Universal Dependency (UD) set](https://universaldependencies.org/). This set, provides an inventory of the dependency relations in human language. In this tutorial, we focus on English and will use this set for the dependency parsing and the relation extraction in the following section. See example: \n",
    "\n",
    "<img src = https://d3i71xaburhd42.cloudfront.net/273f54ea6f3631a78d9dd442609bb2033cfb1ffe/3-Figure14.2-1.png style=\"height:400px\"> Source: Jurafsky, D., & Martin, J.H. Speech and Language Processing. Dependency Parsing (p.275)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1.3 Task 3: Analyze Dependency Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this task we have three sentences with complex sturcture. We want to figure out the root of the entire sentence and the head of each words. Be sure to be familiar yourself with the Universal dependency tags. \n",
    "\n",
    "* *I remember that you have given Tom a gift* \n",
    "* *Bell makes and distributes computer products*\n",
    "* *The NIH is located in Maryland, U.S. and it contains nearly 1,000 scientists and support staff.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### How to solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Get the dependency tags of each word and its head or dependents, if any, using spaCy. \n",
    "```python\n",
    "    # tok.dep_ gives the dependency function that the word plays, \n",
    "    # tok.head gives the head of the current word, notice that the head of the head verb in a sentence is itself\n",
    "    # tok.children gives all the dependents this word has. \n",
    "    # tok.rights, tok.lefts give you the dependents on its right and left\n",
    "    doc = nlp(text)\n",
    "    for tok in doc:\n",
    "        print(tok.dep_)\n",
    "        print(tok.head)\n",
    "        print(tok.children)\n",
    "        print(tok.rights)\n",
    "        print(tok.lefts)\n",
    "```\n",
    "* Visualize the dependency structure, specify the visualization style you want to present, which here is dependency \"dep\"\n",
    "```python\n",
    "    displacy.render(doc, style=\"dep\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "# !pip install benepar   # you may need to install the package if you don't have that\n",
    "import en_core_web_sm\n",
    "from spacy import displacy # import visualization tool\n",
    "\n",
    "# load English language model\n",
    "nlp = en_core_web_sm.load()\n",
    "# Set up the visualization options\n",
    "options = {\"compact\": True, \"bg\": \"#ffffff\",\n",
    "           \"color\": \"black\", \"font\": \"Source Sans Pro\", \"distance\": 100}\n",
    "\n",
    "TEXT_SAMPLE = [\"I remember that you have give Tom a gift\",\n",
    "               \"Bell makes and distributes computer products\",\n",
    "               \"The NIH is located in Maryland, U.S. and it contains nearly 1,000 scientists and support staff\"] \n",
    "\n",
    "for text in TEXT_SAMPLE:\n",
    "    \n",
    "    # Create a dataframe for an easy-to-see output\n",
    "    df = pd.DataFrame(columns = [\"word\", \"dependent tag\", \"head\", \"dependents\", \"left dependents\", \"right dependents\"])\n",
    "    # Import the text and get nlp object\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Here parse the examples  using displacy.render method\n",
    "    displacy.render(doc, style=\"dep\", options=options)\n",
    "    \n",
    "    # Obtain the head and dependents of each word\n",
    "    for tok in doc:\n",
    "        df = df.append([{\n",
    "            \"word\":tok.text,\n",
    "            \"dependent tag\": tok.dep_,\n",
    "            \"head\":tok.head,\n",
    "            \"dependents\":list(tok.children),\n",
    "            \"left dependents\":list(tok.rights),\n",
    "            \"right dependents\":list(tok.lefts)\n",
    "        }])\n",
    "        \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Practice Exercise (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on the task above, can you parse and visualize the dependency structure of the following sentence. You can also choose other tools and text that you are interested in to do the exercise. \n",
    "\n",
    "    \"Marry was looking for her bag but nothing was founded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "\n",
    "text = \"Marry was looking for her bag but nothing was founded\"\n",
    "\n",
    "# here is your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.2 Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the above section we discussed the dependency structure. It seems that we can use the syntactic dependency to extract the predicate-argument relation by traversing the dependency tree. In this part we will be discussing how we can leverage such dependency structure to figure out the **predicates and arguments** relation. \n",
    "This *Triple* relation consists of the entity pairs and their semantic relations, i.e., (Subject, Predicate, Object). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"705\" height=\"537\" src=\"https://www.youtube.com/embed/0pMLy8grLqo\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"705\" height=\"537\" src=\"https://www.youtube.com/embed/0pMLy8grLqo\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Task 4: Extract Subject-Predicate-Object Relation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the following, we will use spaCy to extract such relations. In the above example, \n",
    "\n",
    "* *I remember that you have give Tom a gift.*\n",
    "\n",
    "* *Bell makes and distributes computer products.* \n",
    "\n",
    "* *The NIH is located in Maryland, U.S. and it contains nearly 1,000 scientists and support staff*\n",
    "\n",
    "The expected outcome will be:\n",
    "\n",
    "```python\n",
    "([I], remember, [given])\n",
    "([you], given, [Tom, gift])\n",
    "([Bell], makes, [products])\n",
    "([Bell], distributes, [products])\n",
    "([], located, [NIH])\n",
    "([it], contains, [scientists, staff])\n",
    "```\n",
    "Before we dive into the code detail, feel free to play with the code by running **Code** cell and see what are the outcomes of the above tested codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### How to solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Considering this implementation is a little bit complex, we will take one step at a time. \n",
    "\n",
    "* Import packages and initiate variables\n",
    "* Obtain all verbs of a sentence\n",
    "* Recognize subjects\n",
    "* Recognize objects\n",
    "* Recognize other subjects or objects linked with conjunctions\n",
    "\n",
    "The basic coding logic is to create the function that take the input text and output the results, **getRelation(sent_string)**. Then we create three functions that play roles in the getRelation function, i.e.,  **getSubj(verb)**, **getObj(verb)** and **getConj(word)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 1. Import packages and initiate variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use these constant variables to select the word with targeted dependency labels. The variables includes typical dependency tags of subject and object. In addition to subject and object, we also need dependency tag of the conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import spacy                             #import spacy module\n",
    "import en_core_web_sm                    #import language model\n",
    "\n",
    "from spacy.util import filter_spans      #import filter_spans to avoid duplicate matches\n",
    "from spacy.matcher import Matcher        #import Matcher object to perform regex matching\n",
    "nlp = en_core_web_sm.load()              #load English language model\n",
    "\n",
    "# All possible dependency tags of subject\n",
    "SUBJECTS_DEP = [\"nsubj\",  \"csubj\", \"expl\"]\n",
    "\n",
    "# Subjects with passive voice\n",
    "PASSIVE_SUBJ_DEP = [\"nsubjpass\", \"csubjpass\"]\n",
    "\n",
    "# All possible dependency tags of object\n",
    "OBJECTS_DEP = [\"dobj\", \"dative\", \"pobj\", \"oprd\", ]\n",
    "\n",
    "# Conjunction dependency tags\n",
    "CONJ_DEP = [\"cc\", \"conj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 2. Obtain all the verbs of a sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After we set up the initial settings. We want to constuct a function **getRelation(sent_string)** which will help us extract the relation. In this function, we will idenfify the verb, subject and object from the sentence. We obtain all the verbs of a sentence which is normally the root of a sentence and thus helps us find other sentence parts. We will use the following code to extract the verbs. word.pos_is the pos tag of each word, word.dep_ is the dependency tags of words. \n",
    "\n",
    "```python\n",
    "def getRelation(sent_string):\n",
    "    \n",
    "    doc = nlp(sent_string)\n",
    "    verb_rel_list = [word for word in doc \n",
    "                             if word.pos_ == 'VERB' and word.dep_ not in ['aux', 'auxpass']]\n",
    "```\n",
    "After this extraction, we take the verb as the argument of the function **getSubj(verb)** and **getObj(verb)** to get the subjects and objects.\n",
    "\n",
    "```python\n",
    "\n",
    "    # get the subjects and objects of this verb\n",
    "    tuple_list = []\n",
    "    # here get all verbs  \n",
    "    if verb_rel_list:\n",
    "        for verb in verb_rel_list:\n",
    "            subj = getSubj(verb)\n",
    "            obj = getObj(verb)\n",
    "            print(\"{}\".format((subj, verb, obj)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 3. Recognize subjects: getSubj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* First add all words with the subject dependency labels.\n",
    "\n",
    "```python\n",
    "    subjs = []\n",
    "    subjs.extend([w for w in verb.lefts if w.dep_ in SUBJECTS_DEP and w.pos_ != \"DET\"])\n",
    "```\n",
    "\n",
    "* However, this cannot include all cases. There are several cases to consider: \n",
    "    1. Passive voice, where the subject and object swap their places\n",
    "    2. Conjunction, where two verbs may share the same subject. This situation can be recursive, which means, multiple verbs can be possible --> *I create, implement, and revise the product*. In this case, the 'create' is the head of 'implement', the 'implement' is the head of 'revise'. It thus would be better to use recursive function to iteratively find the head of the verb. \n",
    "    3. [clausal complement (ccomp, xcomp)](https://universaldependencies.org/u/dep/ccomp.html), such as 'I remembered to give...', 'She started to cry'. Both cases contain two verbs and the latter one (\"give\", \"cry\") is the dependent of the first one (\"remembered\", \"started\") which is the head (root) of the sentence.\n",
    "\n",
    "```python\n",
    "def getSubj(verb, limit_time = 3):\n",
    "    \n",
    "    subjs.extend(list(w.rights)[0] for w in verb.rights if w.dep_ == 'agent')\n",
    "    if len(subjs) == 0 and limit_time>0:\n",
    "        limit_time -= 1    \n",
    "        subjs.extend(getSubj(verb.head, limit_time))\n",
    "    else: \n",
    "        print(\"No subject identified: \", verb)\n",
    "```\n",
    "\n",
    "* In some cases a subject may include a conjunction part, such as apple and orange. This is also the subject. Therefore we construct another function getConj(word) which helps us find their subject \"friends\". For example, apple, orange and peach\n",
    "\n",
    "```python\n",
    "    subjs.extend([w for subj in subjs for w in getConj(subj)])\n",
    "    \n",
    "    return subjs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 4. Recognize objects: getObj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* First add all words with object dependency labels\n",
    "```python\n",
    "    objs.extend([w for w in verb.rights if w.dep_ in OBJECTS_DEP])\n",
    "```\n",
    "* However, this cannot include all cases. There are several cases we need to consider:\n",
    "    1. Passive voice, where the subject and object swap their places\n",
    "    2. [clausal complement (ccomp, xcomp)](https://universaldependencies.org/u/dep/ccomp.html), such as 'He said that', 'I remember that'\n",
    "    3. Conjunction and prepostion phrase condition where the objects do not follow the verb directly. The 'relcl' is the conjunction condition where the current verb has a head which is actually the root of the sentence, e.g., create and implement. The 'implement' is the relcl of the 'create'. In prepostion phrase the object follows a preposition, rather than the verb, such as 'look for'. \n",
    "    \n",
    "```python\n",
    "\n",
    "    # Check if passive objects in left children if so the object will be the subject\n",
    "    objs.extend([w for w in verb.lefts if w.dep_ in PASSIVE_SUBJ_DEP])\n",
    "    \n",
    "    # Check complement clause, conjunction and prepostion condition\n",
    "    if len(objs) == 0:\n",
    "        # If the verb is relcl to the main verb, its head is its object, e.g., \"I saw the book you bought(relcl)\". Another example is \"the elements connected (acl) by a link\"  \n",
    "        \n",
    "        if verb.dep_ in [\"relcl\", \"acl\"] and verb.tag_ in [\"VBN\"]:\n",
    "            objs.append(verb.head)    \n",
    "        else:\n",
    "            for child in verb.rights:\n",
    "                \n",
    "                # Consider the clausal complement, get the action of the clause, which can be represented by the verb, \n",
    "                # such as I remember that she cried (remember, cry)\n",
    "                if child.dep_ in ['ccomp', 'xcomp']:\n",
    "                    objs.extend([child])\n",
    "                    break\n",
    "                \n",
    "                # Consider verb_prep condition where prep has the obj child, such as depends on\n",
    "                elif child.pos_ == 'ADP' and child.dep_ == 'prep':\n",
    "                    temp = [w_child for w_child in child.rights if w_child.dep_ in OBJECTS_DEP]\n",
    "                    if temp:\n",
    "                        objs.extend(temp)  \n",
    "                        break          \n",
    "                # Get the verb's child to check dependent verb\n",
    "                elif child.pos_ == 'VERB': \n",
    "                    temp = getObj(child)\n",
    "                    if temp:\n",
    "                        objs.extend(temp)\n",
    "                        break \n",
    "```\n",
    "* In some cases an object may include a conjunction part, such as apple, orange and peach. This is also the object. \n",
    "\n",
    "```python\n",
    "    objs.extend(w for obj in objs for w in getConj(obj))\n",
    "    \n",
    "    return objs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 5. Get the subject's or object's \"friends\": getConj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "* Here we construct the getConj function to capture the conjunction components, such as \"apple, orange and peach\"\n",
    "\n",
    "```python\n",
    "\n",
    "def getConj(word):\n",
    "    '''\n",
    "    Return the conjunction part of a token\n",
    "    '''\n",
    "    return [rchild for rchild in word.rights if rchild.dep_ == 'conj']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import spacy                             #import spacy module\n",
    "import en_core_web_sm                    #import language model\n",
    "\n",
    "from spacy.util import filter_spans      #import filter_spans to avoid duplicate matches\n",
    "from spacy.matcher import Matcher        #import Matcher object to perform regex matching\n",
    "nlp = en_core_web_sm.load()              #load English language model\n",
    "\n",
    "# All possible dependency tags of subject\n",
    "SUBJECTS_DEP = [\"nsubj\",  \"csubj\", \"expl\"]\n",
    "\n",
    "# Subjects with passive voice\n",
    "PASSIVE_SUBJ_DEP = [\"nsubjpass\", \"csubjpass\"]\n",
    "\n",
    "# All possible dependency tags of object\n",
    "OBJECTS_DEP = [\"dobj\", \"dative\", \"pobj\", \"oprd\", ]\n",
    "\n",
    "# Conjunction dependency tags\n",
    "CONJ_DEP = [\"cc\", \"conj\"]\n",
    "\n",
    "\n",
    "# Obtain subjects from the verb\n",
    "def getSubj(verb, limit_time = 3):\n",
    "    '''\n",
    "    Traverse the relation's dependency tree to collect subject\n",
    "        \n",
    "    '''\n",
    "    subjs = []\n",
    "    # check if conjunction (verb as conj to main sentence)\n",
    "    # check if verb's head is verb, if true, then call getSubj\n",
    "    \n",
    "    subjs.extend([w for w in verb.lefts if w.dep_ in SUBJECTS_DEP and w.pos_ != \"DET\"])\n",
    "    \n",
    "    # Check passive tone, if agent in the sentence, i.e., \"by\", collect agent's child as subject\n",
    "    subjs.extend(list(w.rights)[0] for w in verb.rights if w.dep_ == 'agent')\n",
    "\n",
    "    \n",
    "    if len(subjs) == 0 and verb.text != verb.head.text:\n",
    "            \n",
    "        #If verb has no subject, then trace back to its head verb use the subject in main sentence\n",
    "        limit_time -= 1    \n",
    "        \n",
    "        # recursively use this function to further find subject\n",
    "        subjs.extend(getSubj(verb.head, limit_time))\n",
    "        \n",
    "\n",
    "    # Get the shared dependency subject with the ones already identified in conjunction\n",
    "    # Obtain conjunct dependents of the leftmost conjunct, apple, orange and peach\n",
    "    subjs.extend([w for subj in subjs for w in getConj(subj)])\n",
    "    \n",
    "    return subjs\n",
    "\n",
    "def getObj(verb):\n",
    "    '''\n",
    "    Traverse the relation's dependency tree to collect objects\n",
    "    '''\n",
    "    \n",
    "    # If there is only one verb in sentence\n",
    "    objs = []\n",
    "    \n",
    "    # Get the right children dependency of this verb\n",
    "    right_child = [w for w in verb.rights]\n",
    "    \n",
    "    # Collect objects\n",
    "    objs.extend([w for w in verb.rights if w.dep_ in OBJECTS_DEP])\n",
    "    \n",
    "    # here check agent by \n",
    "    # Check if passive objects in left children if so the object will be the subject\n",
    "    objs.extend([w for w in verb.lefts if w.dep_ in PASSIVE_SUBJ_DEP])\n",
    "                \n",
    "    # Check prepostion and conjunction condition\n",
    "    if len(objs) == 0:\n",
    "        # If the verb is relcl to the main verb, its head is its object, e.g., I saw the book you bought(relcl), or elements connected (acl) by a link    \n",
    "        if verb.dep_ in [\"relcl\", \"acl\"] and verb.tag_ in [\"VBN\"]:\n",
    "            objs.append(verb.head) \n",
    "            \n",
    "        else:\n",
    "            for child in verb.rights:\n",
    "                \n",
    "                # Consider the clausal complement, get the action of the clause, which can be represented by the verb, \n",
    "                # such as I remember that she cried (remember, cry)\n",
    "                if child.dep_ in ['ccomp', 'xcomp']:\n",
    "                    objs.extend([child])\n",
    "                    break\n",
    "                \n",
    "                # Consider verb_prep condition where prep has the obj child, such as depends on\n",
    "                elif child.pos_ == 'ADP' and child.dep_ == 'prep':\n",
    "                    temp = [w_child for w_child in child.rights if w_child.dep_ in OBJECTS_DEP]\n",
    "                    if temp:\n",
    "                        objs.extend(temp)  \n",
    "                        break         \n",
    "                \n",
    "                # Get the verb's child to check the dependent verb that share the same objects, e.g., make and develop software\n",
    "                elif child.pos_ == 'VERB': \n",
    "                    temp = getObj(child)\n",
    "                    if temp:\n",
    "                        objs.extend(temp)\n",
    "                        break              \n",
    "    \n",
    "    # Get the shared dependency subject with the ones already identified in conjunction\n",
    "    # Obtain conjunct dependents of the rightmost conjunct, apple, orange and peach\n",
    "    objs.extend(w for obj in objs for w in getConj(obj))   \n",
    "    \n",
    "    return objs\n",
    "\n",
    "def getConj(word):\n",
    "    '''\n",
    "    Return the conjunction part of a token\n",
    "    '''\n",
    "    return [rchild for rchild in word.rights if rchild.dep_ == 'conj']\n",
    "\n",
    "\n",
    "def getRelation(sent_string):\n",
    "    \"\"\"\n",
    "    Obtain S-V-O tuple from the string\n",
    "    Arg: \n",
    "        sent string: A sentence string\n",
    "    Return: A S-V-O tuple list, e.g., [(subject, object, relation), (subject, object, relation)]\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    svo_tuple = []    # a list of tuple (subj, v, obj)\n",
    "    doc = nlp(sent_string)\n",
    "\n",
    "    # Use regex to collect the verb entity which represent relations, except the auxiliry part, am, is, are, can.     \n",
    "#     special_pattern = [{'DEP': \"auxpass\"}, \n",
    "#                        {'POS': \"VERB\"},\n",
    "#                        {'DEP': \"prep\", \"POS\": \"ADP\"}]\n",
    "\n",
    "    # Get the verb and conjunction word match\n",
    "\n",
    "    verb_rel_list = [word for word in doc \n",
    "                                     if word.pos_ == 'VERB' and word.dep_ not in ['aux', 'auxpass']]\n",
    "    \n",
    "    is_rel_list = [word for word in doc \n",
    "                                     if word.pos_ == 'VERB' and word.dep_ not in ['aux', 'auxpass']]\n",
    "\n",
    "    # get the subjects and objects of this verb\n",
    "    tuple_list = []\n",
    "    # here get all verbs  \n",
    "    if verb_rel_list:\n",
    "        for verb in verb_rel_list:\n",
    "            subj = getSubj(verb)\n",
    "            obj = getObj(verb)\n",
    "            print(\"{}\".format((subj, verb, obj)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Play with the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can play with the code before you dive into the detail. Please run the **Code** section first and run the following cell. Replace the text in \n",
    "```python\n",
    "getRelation(text)\n",
    "```\n",
    "with the text you are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TEXT_SAMPLE = \"\"\"\n",
    "I remember that you have given Tom a gift.\n",
    "Bell makes and distributes computer products.\n",
    "The NIH is located in Maryland, U.S. and it contains nearly 1,000 scientists and support staff.\n",
    "\"\"\"\n",
    "CHALLENGE_TEXT = \"\"\"\n",
    "Marry was looking for her bag but nothing was founded\n",
    "\"\"\"\n",
    "\n",
    "getRelation(TEXT_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Taylor A., Marcus M., Santorini B., The Penn Treebank: An Overview (2003), Text, Speech and Language Technology, vol 20 \n",
    "https://www.nltk.org/book/ch05.html   \n",
    "https://www.nltk.org/book/ch07.html   \n",
    "https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf   \n",
    "https://www.kaggle.com/nltkdata/maxent-ne-chunker  \n",
    "https://arxiv.org/ftp/arxiv/papers/1308/1308.0661.pdf   \n",
    "https://courses.cs.washington.edu/courses/cse517/13wi/slides/cse517wi13-RelationExtraction.pdf  \n",
    "https://nlp.stanford.edu/software/dependencies_manual.pdf  \n",
    "https://universaldependencies.org/en/dep/xcomp.html  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
